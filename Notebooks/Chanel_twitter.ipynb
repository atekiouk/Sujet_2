{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e93a2d",
   "metadata": {},
   "source": [
    "## Chanel Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c602a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_31804\\2998298859.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  chanel_twitt = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/twitt_chanel.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chanel_twitt = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/twitt_chanel.csv\",sep=\";\", error_bad_lines=False)\n",
    "chanel_t = chanel_twitt[chanel_twitt['language'] == 'en']\n",
    "pub_t = chanel_twitt[chanel_twitt['language'] == 'en']['publication_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d86c7",
   "metadata": {},
   "source": [
    "#### Ajout des token 'hashtag' et 'arobase' dans la pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "964d4b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x29ce8e34290>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacymoji import Emoji\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "656d15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"at\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_at = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '@':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_at = True\n",
    "                        break\n",
    "        if not merged_at:\n",
    "            break\n",
    "        merged_at = False\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c268778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.hashtag_pipe(doc)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"at\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8718e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "hashtag_getter = lambda token: token.text[0] in (\"@\")\n",
    "Token.set_extension(\"is_at\", getter=hashtag_getter, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f851325",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d3ef745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.hashtag_pipe(doc)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"hashtag\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86399336",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_getter = lambda token: token.text[0] in (\"#\")\n",
    "Token.set_extension(\"is_hashtag\", getter=hashtag_getter, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a149e5",
   "metadata": {},
   "source": [
    "TOP hasthags TOP emojis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94741b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def top_hashtags(text, N, top):\n",
    "    hashtag = []\n",
    "    index_vector = np.random.randint(len(text), size=N)\n",
    "    for i in index_vector:\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            if token._.is_hashtag:\n",
    "                hashtag.append(token.text)\n",
    "    Hashtag_corpus = pd.DataFrame()\n",
    "    Hashtag_corpus['hashtag'] = hashtag\n",
    "    Hashtag_corpus2 = Hashtag_corpus.groupby(\"hashtag\").size()\n",
    "    print(Hashtag_corpus2.nlargest(top).tail(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3ee4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_emojis(text, N, top):\n",
    "    emoji = []\n",
    "    index_vector = np.random.randint(len(text), size=N)\n",
    "    for i in index_vector:\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            if token._.is_emoji:\n",
    "                emoji.append(token.text)\n",
    "    Emoji_corpus = pd.DataFrame()\n",
    "    Emoji_corpus['emoji'] = emoji\n",
    "    Emoji_corpus2 = Emoji_corpus.groupby(\"emoji\").size()\n",
    "    print(Emoji_corpus2.nlargest(top).tail(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc8ed328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag\n",
      "#JENNIE            62\n",
      "#Blackpink         15\n",
      "#Jennie            13\n",
      "#gdragon           10\n",
      "#chanel             8\n",
      "#Poshmark           7\n",
      "#shopmycloset       7\n",
      "#VisibleWomen       6\n",
      "#fashion            5\n",
      "#Chanel             4\n",
      "#Ridiculousness     4\n",
      "#angelawhite        4\n",
      "#b                  4\n",
      "#beauty             4\n",
      "#cosme              4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_hashtags(chanel_t['text'].tolist(),1000,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4be06b92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "üÖæ     92\n",
      "üò≠     79\n",
      "üìç     59\n",
      "üîê     54\n",
      "üá∞üá∑    32\n",
      "üòç     27\n",
      "üîπ     24\n",
      "‚ù§Ô∏è    19\n",
      "üëá     18\n",
      "ü§≠     18\n",
      "ü•∞     16\n",
      "üñ§     11\n",
      "üòÇ     11\n",
      "ü§î     10\n",
      "‚ú®      9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_emojis(chanel_t['text'].tolist(), 1000, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27a43f",
   "metadata": {},
   "source": [
    "### Constitution d'un corpus afin de d√©finir les junk post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3283bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_hashtag(t): #--text\n",
    "    hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "    return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #\n",
    "def clean_at(t):\n",
    "    at_pattern= re.compile(\"@[A-Za-z0-9_]+\")\n",
    "    return re.sub(at_pattern,\"\", t)\n",
    "\n",
    "\n",
    "def del_double(txt,publication_time,s,method): # --text --liste des date et heure de publication pour chaque √©l√©ment de la liste --seuil (s dans [0;1] ou [0;100] selon la m√©thode) --m√©thode\n",
    "    t = txt.copy()\n",
    "    distance = method #initialisiation de la fonction de distance\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        j=i+1\n",
    "        while(j<r):\n",
    "            if(distance(clean_at(clean_hashtag(t[i])).strip(),clean_at(clean_hashtag(t[j])).strip()) <= s ): # Si la distance entre les deux √©lemens de la liste inf √† seuil\n",
    "                if(publication_time[i]<publication_time[j]):\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    del t[i]\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "            else:\n",
    "                j+=1\n",
    "        i+=1\n",
    "    return t\n",
    "#passer en param√®tre l'objet distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee10d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "dist = textdistance.levenshtein.normalized_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dec15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "chanel_t_junk_valid = random.choices(chanel_t['text'].tolist(), k=300)\n",
    "chanel_t_junk_valid = del_double(chanel_t_junk_valid, pub_t.tolist(),0.6,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edcc7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chanel_t_junk_valid).to_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_t_junk_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc26df9",
   "metadata": {},
   "source": [
    "Exemple de requ√™te twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75892c",
   "metadata": {},
   "source": [
    "`to:NASA porque -argentina lang:es`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
