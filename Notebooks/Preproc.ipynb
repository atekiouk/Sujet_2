{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7971fb0",
   "metadata": {},
   "source": [
    "# Preprocessing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56e37b",
   "metadata": {},
   "source": [
    "corpus de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "977c8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_14604\\3058205479.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n",
    "text = corpus_sephora3k[corpus_sephora3k['language'] == 'en']['text'].tolist()\n",
    "publication_time = corpus_sephora3k[corpus_sephora3k['language'] == 'en']['publication_time'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b154b8b",
   "metadata": {},
   "source": [
    "#### Initialisation pipe spaCy en anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5722e",
   "metadata": {},
   "source": [
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421ccea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f77d0",
   "metadata": {},
   "source": [
    "#### Extraction des emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191fb46",
   "metadata": {},
   "source": [
    "Ajout de **emoji** dans la pipe spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d1c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x24ee7d38580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacymoji import Emoji\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72187ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clarascanner/blob/main/notebooks/cleaning.ipynb\n",
    "\n",
    "\n",
    "#------------------------ /!\\ Ne fonctionne pas------------------------------\n",
    "from emoji import UNICODE_EMOJI \n",
    "\n",
    "def emojis_extraction(text):\n",
    "    emojis = []\n",
    "    for x in text:\n",
    "        if x in UNICODE_EMOJI['en']:\n",
    "            emojis.append(x)\n",
    "            text = text.replace(x, f' {x} ')\n",
    "    return (text,emojis)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "import re\n",
    "def clean_emojis(txt): #--chaine de caractères\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "            u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U0001F1F2\"\n",
    "            u\"\\U0001F1F4\"\n",
    "            u\"\\U0001F620\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100cfe",
   "metadata": {},
   "source": [
    "#### Identification des doublons\n",
    "Afin d'éviter d'analyser le texte de post identiques, nous allons ici supprimer les doublons présent dans notre corpus de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cc3d5",
   "metadata": {},
   "source": [
    "Pour ce faire nous allon utiliser la librairie textdistance (`pip install text-distance`), et nottament la distance normalisée `textdistance.hamming.normalized_distance` qui nous permettra de définir un seuil entre 0 et 1.\n",
    "Nous pouvons également utiliser la librairie `fuzzywuzzy`, qui va nous permettre grâce à la fonction *token_sort_ratio* de mesurer la similitude entre deux chaines de caractères malgré la position des mots dans la chaines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15544dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea61527",
   "metadata": {},
   "source": [
    "On doit enlever les hashtags temporairement afin de vérifier s'il existe des doublons, car en effet des hashtag différent sur deux posts qui sont en fait identiques peuvent perturber le tri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea7e57",
   "metadata": {},
   "source": [
    "Fonction qui retourne notre texte sans les # : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a33b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(t): #--text\n",
    "    hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "    return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f309c",
   "metadata": {},
   "source": [
    "Une fois les hashtags retiré, il nous faut calculer la distance entre tout les éléments du corpus. La fonction suivante va calculer la distance entre les élements de la liste (corpus de texte) et ensuite supprimer les élements qui sont présent plusieurs fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d60ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3569005658.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    distance = method #initialisiation de levenshtein avec la distance normalisée.\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def del_double(txt,publication_time,s,method): # --text --liste des date et heure de publication pour chaque élément de la liste --seuil (s dans [0;1] ou [0;100] selon la méthode) --méthode\n",
    "    t = txt.copy()\n",
    "        distance = method #initialisiation de levenshtein avec la distance normalisée.\n",
    "        i = 0\n",
    "        r = len(t)\n",
    "        while(i<r):\n",
    "            r = len(t)\n",
    "            j=i+1\n",
    "            while(j<r):\n",
    "                if(distance(clean_hashtag(t[i]).strip(),clean_hashtag(t[j]).strip()) <= s ): # Si la distance entre les deux élemens de la liste inf à seuil\n",
    "                    if(publication_time[i]<publication_time[j]):\n",
    "                        del t[j] #delete\n",
    "                        r = len(t) #on actualise la taille de la listes\n",
    "                    else:\n",
    "                        del t[i]\n",
    "                        r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    return t\n",
    "#passer en paramètre l'objet distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326d6af",
   "metadata": {},
   "source": [
    "Faut-il enlever temporairement les emojis avant d'identifier les doublons ?   \n",
    "Test sur 100 posts  \n",
    "Nombre de post restant après dédoublonnage sans emojis : 93  \n",
    "Nombre de post restant après dédoublonnage avec emojis : 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40280205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les doublons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a35428",
   "metadata": {},
   "source": [
    "#### Ratio mot/hashtag\n",
    "Calcul du ratio mot/hastag dans un post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3b69",
   "metadata": {},
   "source": [
    "Ajout du pattern de token 'hashtag' dans la pipe du modèle spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eb323",
   "metadata": {},
   "source": [
    "On ajoute la méthode **is_hashtag** sur l'objet *token*, cette méthode va nous renvoyer ***True*** si le token est un hasgtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97c044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_getter = lambda token: token.text[0] in (\"#\")\n",
    "Token.set_extension(\"is_hashtag\", getter=hashtag_getter, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ab37d",
   "metadata": {},
   "source": [
    "Afin de calculer le ration mot/hashtags, nous allons claculer le nombre de mot, cependant en utilisant la pipe spacy classique, un '#' est un token en lui-même. Nous devons donc calculer le nom de de '#' et le soustraire au nombre total de mot, car en effet tout les mot précédé d'un '#' seront considéré comme de \"vrai' mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e251298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "def del_junk_post(txt,s): # --text --seuil (s dans [0;1], le seuil 1 permet de supprimer un post composé à 100% de hashtag)\n",
    "    t = txt.copy()\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        nb_hash = 0\n",
    "        nb_word = 0\n",
    "        doc = nlp(t[i]) # On tokenize l'element de notre liste\n",
    "        for token in doc: # Pour chaque token de la liste\n",
    "            if(token._.is_hashtag):\n",
    "                nb_hash+=1\n",
    "            else:\n",
    "                nb_word+=1\n",
    "        if(nb_word==0): # On vérifie d'abord que le nombre de mot n'est pas nul, auquel cas nous supprimmons le post\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        elif(nb_hash/(nb_word) > s): # On calcul le ratio, si inférieur au seuil => on delete\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        i+=1\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ff4ad",
   "metadata": {},
   "source": [
    "#### Constitution d'un échantillon permettant de determiner les caractéristiques d'un junk post (pub ou autre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c94ffafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.randint(1992, size = 400)\n",
    "t_array = np.array(text)\n",
    "t_junk = t_array[sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9bebaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(t_junk).to_csv( \"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_junk.csv\" ,encoding='utf-8')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e329011",
   "metadata": {},
   "source": [
    "#### Traitement des hashtag restant et des emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f74223bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0258ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'i', 'love', 'sep', 'hora']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('helloilovesephora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ad791",
   "metadata": {},
   "outputs": [],
   "source": [
    " #/!\\ Custom le language en ajoutant les marques !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f0b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e107723",
   "metadata": {},
   "source": [
    "check les seuils , décomposer les mot dans les hashtag, position du hashtag dans le post, dictionnaire frequence langue anglaise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1fa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e48141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c4063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8960de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0a77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59535933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61459d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508540cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bda77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20096786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe47b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa9b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d04cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49212a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
