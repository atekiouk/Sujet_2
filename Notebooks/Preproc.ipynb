{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7971fb0",
   "metadata": {},
   "source": [
    "# Preprocessing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56e37b",
   "metadata": {},
   "source": [
    "corpus de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977c8860",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_22620\\735203815.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n",
    "corpus_sephora3k.head()\n",
    "text = corpus_sephora3k[corpus_sephora3k['language'] == 'en']['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b154b8b",
   "metadata": {},
   "source": [
    "#### Initialisation pipe spaCy en anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5722e",
   "metadata": {},
   "source": [
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421ccea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f77d0",
   "metadata": {},
   "source": [
    "#### Extraction des emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191fb46",
   "metadata": {},
   "source": [
    "Ajout de **emoji** dans la pipe spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d1c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x2026d805300>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacymoji import Emoji\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72187ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clarascanner/blob/main/notebooks/cleaning.ipynb\n",
    "\n",
    "\n",
    "#------------------------ /!\\ Ne fonctionne pas------------------------------\n",
    "from emoji import UNICODE_EMOJI \n",
    "\n",
    "def emojis_extraction(text):\n",
    "    emojis = []\n",
    "    for x in text:\n",
    "        if x in UNICODE_EMOJI['en']:\n",
    "            emojis.append(x)\n",
    "            text = text.replace(x, f' {x} ')\n",
    "    return (text,emojis)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "import re\n",
    "def clean_emojis(txt): #--chaine de caractères\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "            u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U0001F1F2\"\n",
    "            u\"\\U0001F1F4\"\n",
    "            u\"\\U0001F620\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100cfe",
   "metadata": {},
   "source": [
    "#### Identification des doublons\n",
    "Afin d'éviter d'analyser le texte de post identiques, nous allons ici supprimer les doublons présent dans notre corpus de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cc3d5",
   "metadata": {},
   "source": [
    "Pour ce faire nous allon utiliser la librairie textdistance (`pip install text-distance`), et nottament la distance normalisée `textdistance.hamming.normalized_distance` qui nous permettra de définir un seuil entre 0 et 1.\n",
    "Nous pouvons également utiliser la librairie `fuzzywuzzy`, qui va nous permettre grâce à la fonction *token_sort_ratio* de mesurer la similitude entre deux chaines de caractères malgré la position des mots dans la chaines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15544dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea61527",
   "metadata": {},
   "source": [
    "On doit enlever les hashtags temporairement afin de vérifier s'il existe des doublons, car en effet des hashtag différent sur deux posts qui sont en fait identiques peuvent perturber le tri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea7e57",
   "metadata": {},
   "source": [
    "Fonction qui retourne notre texte sans les # : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a33b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(t): #--text\n",
    "    hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "    return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f309c",
   "metadata": {},
   "source": [
    "Une fois les hashtags retiré, il nous faut calculer la distance entre tout les éléments du corpus. La fonction suivante va calculer la distance entre les élements de la liste (corpus de texte) et ensuite supprimer les élements qui sont présent plusieurs fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d60ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ Renvoie une matrice de distance, pas opti\n",
    "#------------------------------------------------------------------------------------------\n",
    "def dist_mat(t): #-- text\n",
    "    dist_matrix = np.zeros((len(t),len(t)),dtype=np.float)\n",
    "    for i in range(len(t)):\n",
    "        for j in range(len(t)):\n",
    "            dist_matrix[i,j] = hamming(t[i],t[j])\n",
    "    return dist_matrix\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def del_double(txt,s,method): # --text --seuil (s dans [0;1] ou [0;100] selon la méthode) --méthode (hamming-levenshtein.normalized_distance ou fuzzywuzzy-token_sort_ratio)\n",
    "    t = txt.copy()\n",
    "    if(method == \"hamming-levenshtein.normalized_distance\"): #Ici deux texte identique dist = 0\n",
    "        hamming = textdistance.hamming.normalized_distance #initialisiation de hamming avec la distance normalisée.\n",
    "        i = 0\n",
    "        r = len(t)\n",
    "        while(i<r):\n",
    "            r = len(t)\n",
    "            j=i+1\n",
    "            while(j<r):\n",
    "                if(hamming(clean_emojis(clean_hashtag(t[i])).strip(),clean_emojis(clean_hashtag(t[j])).strip() <= s ): # Si la distance entre les deux élemens de la liste inf à seuil\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    if(method == \"fuzzywuzzy-token_sort_ratio\"): #Ici deux texte identique ratio = 100\n",
    "        i = 0\n",
    "        r = len(t)\n",
    "        while(i<r):\n",
    "            r = len(t)\n",
    "            j=i+1\n",
    "            while(j<r):\n",
    "                if(token_sort_ratio(clean_emojis(clean_hashtag(t[i])),clean_emojis(clean_hashtag(t[j]))) >= s ): # Si la distance entre les deux élemens de la liste inf à seuil\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    return t\n",
    "\n",
    "# Supprimer les doublons en fonction du timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a35428",
   "metadata": {},
   "source": [
    "#### Ratio mot/hashtag\n",
    "Calcul du ratio mot/hastag dans un post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b42db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3b69",
   "metadata": {},
   "source": [
    "(OPTIONEL) Ici nous ajoutons le token 'hashtag' dans la pipe du modèle spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6a638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  /!\\ Ne marche pas\n",
    "#--------------------------------------------------------------------------------------------\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    print(start_index, end_index)\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        if retokenizer.merge(doc[start_index:end_index]) is not None: #PB avec retokenize\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d29fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.hashtag_pipe(doc)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b892e20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E199] Unable to merge 0-length span at `doc[3:3]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHello #Hello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1016\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1016\u001b[0m     \u001b[43merror_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Doc):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE005\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, returned_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:1672\u001b[0m, in \u001b[0;36mraise_error\u001b[1;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[1;32m-> 1672\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m, in \u001b[0;36mhashtag_pipe\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(start_index, end_index)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mretokenize() \u001b[38;5;28;01mas\u001b[39;00m retokenizer:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mretokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         merged_hashtag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokens\\_retokenize.pyx:55\u001b[0m, in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E199] Unable to merge 0-length span at `doc[3:3]`."
     ]
    }
   ],
   "source": [
    "nlp('Hello #Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ab37d",
   "metadata": {},
   "source": [
    "Afin de calculer le ration mot/hashtags, nous allons claculer le nombre de mot, cependant en utilisant la pipe spacy classique, un '#' est un token en lui-même. Nous devons donc calculer le nom de de '#' et le soustraire au nombre total de mot, car en effet tout les mot précédé d'un '#' seront considéré comme de \"vrai' mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e251298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "def del_junk_post(txt,s): # --text --seuil (s dans [0;1], le seuil 1 permet de supprimer un post composé à 100% de hashtag)\n",
    "    t = txt.copy()\n",
    "    nlp = English()\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        nb_hash = 0\n",
    "        nb_word = 0\n",
    "        doc = nlp(t[i]) # On tokenize l'element de notre liste\n",
    "        for token in doc: # Pour chaque token de la liste\n",
    "            if(token.text == '#'):\n",
    "                nb_hash+=1\n",
    "            else:\n",
    "                nb_word+=1\n",
    "        if(nb_word-nb_hash==0): # On vérifie d'abord que le nombre de mot n'est pas nul, auquel cas nous supprimmons le post\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        elif(nb_hash/(nb_word-nb_hash) > s): # On calcul le ratio, si inférieur au seuil => on delete\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        i+=1\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5bbb3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour à tous', 'Salut', 'tutu tata toto']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = ['a #salut #bonjour #hello', 'Bonjour à tous','je je je je #salut #bonjour #hello','Salut','#bjr #llala #kdkdd', 'tutu tata toto']\n",
    "del_junk_post(tst,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a12d602d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4778179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e329011",
   "metadata": {},
   "source": [
    "#### Traitement des hashtag restant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f74223bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0258ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'i', 'love', 'sep', 'hora']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('helloilovesephora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ad791",
   "metadata": {},
   "outputs": [],
   "source": [
    " #/!\\ Custom le language en ajoutant les marques !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f0b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e107723",
   "metadata": {},
   "source": [
    "check les seuils , décomposer les mot dans les hashtag, position du hashtag dans le post, dictionnaire frequence langue anglaise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1fa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e48141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c4063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8960de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0a77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59535933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61459d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508540cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bda77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20096786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe47b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa9b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d04cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49212a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
