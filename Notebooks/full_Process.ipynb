{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb968f72",
   "metadata": {},
   "source": [
    "# Analyse d'image de marque sur les réseaux sociaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ced35",
   "metadata": {},
   "source": [
    "### Importation des Bibliothèques et du Fichier 'utilities'\n",
    "\n",
    "Dans cette section, nous importons les bibliothèques Python essentielles pour notre analyse de traitement naturel du langage (NLP) et nos tâches de *machine learning*. Ces bibliothèques comprennent des outils pour le prétraitement des données, la modélisation, l'évaluation des modèles, et bien d'autres. Nous avons également créé un fichier nommé 'utilities.py' qui contient des méthodes personnalisées que nous utiliserons tout au long du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7efb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sklearn\n",
    "import random\n",
    "import textdistance\n",
    "import umap\n",
    "import pickle\n",
    "\n",
    "from sklearn.tree              import DecisionTreeClassifier\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn                   import metrics\n",
    "from sklearn.model_selection   import GridSearchCV\n",
    "from transformers              import BertTokenizer, BertModel\n",
    "from sentence_transformers     import SentenceTransformer\n",
    "from sklearn.metrics           import roc_curve\n",
    "from umap                      import UMAP\n",
    "from typing                    import Optional\n",
    "from sklearn.decomposition     import PCA\n",
    "from sklearn.linear_model      import LogisticRegression\n",
    "from sklearn.metrics           import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6930d84",
   "metadata": {},
   "source": [
    "### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel10k.csv\", sep=\";\", parse_dates=[\"publication_time\"])\n",
    "\n",
    "# on récupére les posts en anglais\n",
    "mask = corpus[\"language\"] == 'en'\n",
    "publication_time_chanel = corpus['publication_time'].tolist()\n",
    "\n",
    "print(f\"# documents in corpus: {len(chanel = corpus['text'].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068a797",
   "metadata": {},
   "source": [
    "### Importation du Corpus de Publications Labellisées\n",
    "\n",
    "Dans cette section, nous procédons à l'importation du corpus de 500 publications labellisées comme \"junk\" (indésirables) ou \"non-junk\" (acceptables). Ces données sont essentielles pour notre projet, car elles serviront de base pour l'entraînement et l'évaluation de nos modèles de détection de publications indésirables.  \n",
    "  \n",
    "Avant d'importer les données, nous avons effectué une étape cruciale de prétraitement en utilisant une méthode personnalisée que nous avons définie dans le fichier 'utilities.py'. Cette méthode, appelée `delete_duplicates`, a permis de supprimer efficacement les doublons et les publications quasi-doublons de notre corpus. Cette opération est importante pour éviter tout biais dans notre analyse, car des publications similaires pourraient fausser les résultats de nos modèles.  \n",
    "  \n",
    "Le corpus de données que nous importons ici est prêt à être utilisé pour la caractérisation textuelle, la modélisation, et l'évaluation de nos modèles de détection de publications indésirables. Nous pouvons maintenant procéder à l'exploration de ces données et à la mise en place des étapes suivantes de notre projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new = pd.read_excel('C:/Users/a.tekiouk/Sujet_2/DATA/chanel_junk_valid_2.xlsx')\n",
    "chanel_junk_valid_new = chanel_junk_valid_new[['text','is_junk']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e4dd1",
   "metadata": {},
   "source": [
    "## Caractérisation manuelle du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daeea17",
   "metadata": {},
   "source": [
    "### Création de la Pipeline spaCy 'nlp'\n",
    "\n",
    "Dans cette section, nous mettons en place une pipeline spaCy 'nlp' personnalisée qui nous permettra de traiter et d'analyser nos publications textuelles. SpaCy est une bibliothèque NLP (Traitement du Langage Naturel) puissante et polyvalente.\n",
    "\n",
    "Tout d'abord, nous importons spaCy et initialisons la pipeline 'nlp'. Cette pipeline comprendra plusieurs étapes de prétraitement et d'analyse du texte, ce qui facilitera notre travail d'extraction d'informations et de caractérisation des publications.\n",
    "\n",
    "Une des étapes importantes que nous ajoutons à cette pipeline est la reconnaissance des tokens 'hashtags' et 'emojis'. Les hashtags (#) sont couramment utilisés sur les réseaux sociaux pour identifier des sujets spécifiques, tandis que les emojis ajoutent une dimension émotionnelle à un texte. En identifiant ces éléments, nous pouvons mieux comprendre le contenu des publications, ce qui est essentiel pour notre analyse.\n",
    "\n",
    "Nous personnalisons notre pipeline spaCy pour répondre aux besoins spécifiques de notre projet, ce qui nous permettra d'extraire efficacement les informations pertinentes des publications et de les utiliser dans nos modèles de détection de publications indésirables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(\n",
    "    doc: spacy.tokens.Doc\n",
    ") -> spacy.tokens.Doc:\n",
    "    \"\"\"\n",
    "    A spaCy pipeline component that merges tokens of the form #word into a single token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        The input spaCy Doc object to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spacy.tokens.Doc\n",
    "        The processed spaCy Doc object with hashtags merged into a single token.\n",
    "    \"\"\"\n",
    "    len_doc = -1\n",
    "    for token in doc:\n",
    "        len_doc=len_doc+1\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if(token.head is not None and token.i!=len_doc):\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe(\"hashtag\", first=True)\n",
    "Token.set_extension(\"is_hashtag\", getter=lambda token: token.text[0] in (\"#\"), force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0c743",
   "metadata": {},
   "source": [
    "### Création de Variables Caractéristiques\n",
    "\n",
    "Dans cette section, nous créons plusieurs variables caractéristiques à partir des publications de notre corpus. Ces variables nous permettront de mieux caractériser chaque publication et de les utiliser dans nos modèles de détection de publications indésirables.\n",
    "\n",
    "- **has_URL** : Cette variable binaire indique si une publication contient ou non une URL (Uniform Resource Locator). La présence d'URLs peut être un indicateur de publications promotionnelles ou de spam.  \n",
    "\n",
    "- **has_phone_number** : Cette variable binaire indique si une publication contient ou non un numéro de téléphone. La présence de numéros de téléphone peut être associée à des publications de type publicité ou spam.  \n",
    "\n",
    "- **has_currency_symbol** : Cette variable binaire indique si une publication contient ou non un symbole monétaire (comme le dollar ou l'euro). La présence de symboles monétaires peut être liée à des offres commerciales ou publicitaires.  \n",
    "\n",
    "- **word_ratio** : Cette variable quantifie la répartition des mots par rapport à l'ensemble des éléments textuels (mots, ponctuations, emojis, hashtags, etc.) dans une publication. Elle peut nous donner des indications sur la densité du texte et sa structure. Par exemple, une faible densité de mots par rapport aux autres éléments peut suggérer une publication dominée par des éléments non textuels tels que des emojis et des hashtags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee41eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new = pd.read_excel('C:/Users/a.tekiouk/Sujet_2/DATA/chanel_junk_valid_2.xlsx')\n",
    "chanel_junk_valid_new = chanel_junk_valid_new[['text','is_junk']].dropna()\n",
    "chanel_junk_valid_new['has_URL'] = utilities.get_presence_of_URL(corpus= chanel_junk_valid_new['text'], nlp= nlp  )\n",
    "chanel_junk_valid_new['has_phone_number'] = utilities.get_presence_of_phone_numbers(corpus= chanel_junk_valid_new['text'], nlp= nlp  )\n",
    "chanel_junk_valid_new['has_currency_symbol'] = utilities.get_presence_of_currency_symbol(corpus= chanel_junk_valid_new['text'], nlp= nlp  )\n",
    "chanel_junk_valid_new['word_ratio'] = utilities.get_word_ratio(corpus= chanel_junk_valid_new['text'], nlp= nlp  )\n",
    "chanel_junk_valid_new = chanel_junk_valid_new.join(utilities.create_dummies(corpus= chanel_junk_valid_new['text'],y = chanel_junk_valid_new['is_junk'], element= 'emoji',nlp = nlp, top= 3))\n",
    "chanel_all_features = chanel_junk_valid_new.join(utilities.create_dummies(corpus= chanel_junk_valid_new['text'],y = chanel_junk_valid_new['is_junk'], element= 'hashtag', nlp = nlp, top= 3))\n",
    "X_train, X_test, y_train, y_test = train_test_split(chanel_all_features.dropna().drop(['text','is_junk'],axis=1), chanel_all_features.dropna()['is_junk'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc91260",
   "metadata": {},
   "source": [
    "### Optimisation des Hyperparamètres\n",
    "\n",
    "Dans cette section, nous détaillons le processus d'optimisation des hyperparamètres pour nos modèles de détection de publications indésirables.\n",
    "\n",
    "1. **Optimisation en Fonction de l'AUC**\n",
    "\n",
    "    - Nous commençons par rechercher les meilleures combinaisons d'hyperparamètres pour nos modèles en fonction de l'AUC (Area Under the Curve). L'AUC mesure la capacité d'un modèle à bien classer les observations positives et négatives.\n",
    "    \n",
    "    - Nous utilisons des méthodes d'optimisation telles que la recherche en grille (*GridSearch*) pour explorer différentes valeurs d'hyperparamètres.\n",
    "    \n",
    "    - L'objectif principal est de maximiser l'AUC, ce qui garantit une capacité globale de classification précise de nos modèles.\n",
    "\n",
    "2. **Optimisation en Fonction de la Précision**\n",
    "\n",
    "    - Après avoir obtenu les meilleures configurations d'hyperparamètres en fonction de l'AUC, nous passons à l'optimisation en fonction de la précision.\n",
    "    \n",
    "    - La précision est essentielle pour minimiser les faux positifs, c'est-à-dire les publications incorrectement identifiées comme indésirables.\n",
    "    \n",
    "    - Nous ajustons les hyperparamètres autour des valeurs optimales précédemment trouvées tout en mettant l'accent sur l'amélioration de la précision.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  'criterion': ['entropy'],\n",
    "                'ccp_alpha' : np.linspace(0,0.5,50),\n",
    "                'min_samples_leaf' : np.linspace(2,15,14,dtype= int),\n",
    "                'min_samples_split' : np.linspace(5,25,21,dtype= int)\n",
    "                }  \n",
    "   \n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state = 42), param_grid, scoring = 'roc_auc' ,refit = True, verbose = 3,n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ccp = {  'criterion': ['entropy'],\n",
    "                    'ccp_alpha' : np.linspace(0.0,0.01,40),\n",
    "                    'min_samples_leaf' : np.linspace(3,5,3,dtype= int),\n",
    "                    'min_samples_split' : np.linspace(19,21,3,dtype= int)\n",
    "                    }  \n",
    "\n",
    "   \n",
    "grid_ccp = GridSearchCV(DecisionTreeClassifier(random_state=42),param_grid_ccp, scoring = 'precision' ,refit = True, verbose = 3,n_jobs=-1)\n",
    "grid_ccp.fit(X_train, y_train)\n",
    "print(grid_ccp.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_ccp.best_params_\n",
    "best_params['random_state']=42\n",
    "tree_f = DecisionTreeClassifier(**best_params)\n",
    "tree_f.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75d2a9",
   "metadata": {},
   "source": [
    "*Serialization* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d63cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/a.tekiouk/Sujet_2/Models/model_tree_auc_spacy.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(tree_f, f) # serialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_f.predict(X_test)\n",
    "utilities.evaluate(y_test=y_test,\n",
    "                   y_pred=y_pred,\n",
    "                   y_score=tree_f.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972152da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model_tree_auc_spacy.predict_proba(X_test)[:,1]\n",
    "utilities.plot_roc(y_test,y_score,\"lightblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39b5b1",
   "metadata": {},
   "source": [
    "## Caractérisation automatique du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f436831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_ID = \"all-MiniLM-L6-v2\"\n",
    "encoder = SentenceTransformer(ENCODER_ID)\n",
    "\n",
    "X = encoder.encode(chanel_junk_valid_new['text'])\n",
    "y = chanel_junk_valid_new['is_junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdf23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_auc,best_auc,opt_dim,reducer = utilities.auto_model_selection(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    dimension_range=range(10,160,10))\n",
    "print(f\"best model : {best_model_auc}, hyperparameters = {best_model_auc.get_params()}\\nAUC : {best_auc}\\nOptimal number of dimensions : {opt_dim}\\nBest reducing method : {reducer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733ec8f",
   "metadata": {},
   "source": [
    "*Serialization* : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/a.tekiouk/Sujet_2/Models/model_auc.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(best_model_auc, f) # serialize the model\n",
    "with open('C:/Users/a.tekiouk/Sujet_2/Models/opt_reducer.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(reducer, f) # serialize the reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079e7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d50aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
