{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675abf8a",
   "metadata": {},
   "source": [
    "# Import des Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153ad43",
   "metadata": {},
   "source": [
    "On importe toutes les bibliothèques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9f79094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "from spacy.vocab               import Vocab\n",
    "from spacy.language            import Language\n",
    "from spacy.tokens              import Token\n",
    "from spacymoji                 import Emoji\n",
    "from   sklearn.tree            import DecisionTreeClassifier\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.metrics         import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fcce9",
   "metadata": {},
   "source": [
    "Importation du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3957c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# documents in corpus: 2285\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_5k.csv\", sep=\";\", parse_dates=[\"publication_time\"])\n",
    "\n",
    "# on récupére les posts en anglais\n",
    "mask = corpus[\"language\"] == 'en'\n",
    "corpus = corpus.loc[mask]\n",
    "chanel = corpus['text'].tolist()\n",
    "publication_time_chanel = corpus['publication_time'].tolist()\n",
    "\n",
    "print(f\"# documents in corpus: {len(chanel)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357d64c",
   "metadata": {},
   "source": [
    "On définit les différents éléments de la *pipeline* `spacy` pour détecter les emojis et les hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44faad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(\n",
    "    doc : spacy.tokens.doc.Doc\n",
    ") -> spacy.tokens.doc.Doc:\n",
    "    \"\"\"\n",
    "    Spacy pipeline component that detects if a word is a hashtag or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        The input document to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spacy.tokens.Doc\n",
    "        The processed document with updated token attributes.\n",
    "    \"\"\"\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "# définition du pipe\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe(\"hashtag\", first=True)\n",
    "Token.set_extension(\"is_hashtag\", getter=lambda token: token.text[0] in (\"#\"), force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1fbf0",
   "metadata": {},
   "source": [
    "# Méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcfe3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_trailing_hash(\n",
    "    corpus: list[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Clear any trailing '#' character from each string in a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of str\n",
    "        List of strings to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function only modifies the input corpus list in place.\n",
    "    \"\"\"\n",
    "    for i in range(len(corpus)):\n",
    "        if(corpus[i][-1]==\"#\"):\n",
    "            corpus[i] = corpus[i].rstrip(corpus[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1a3e567",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:27\u001b[1;36m\u001b[0m\n\u001b[1;33m    hashtags = []\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# TODO: voir si l'on peut passer en paramètre un objet spacy qui identifie le type de token,\n",
    "# afin de n'écrire qu'une seule méthode (pour les hashtags, les emojis, etc...).\n",
    "def top_hashtags(\n",
    "    corpus: list,\n",
    "    top: int = 5,\n",
    "    nlp : spacy.lang.en.English\n",
    ") -> pd.Series:\n",
    "        \"\"\"\n",
    "    Retrieves the most frequent hashtags in the given corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list\n",
    "        The list of text documents to retrieve hashtags from.\n",
    "    top : int, optional\n",
    "        The number of top hashtags to return. Default is 5.\n",
    "    nlp : spacy.lang.en.English\n",
    "        A spacy language model with a custom pipe to detect hashtags.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series containing the count of each of the most frequent hashtags found in the corpus,\n",
    "        sorted in descending order.\n",
    "    \"\"\"\n",
    "    # retrieve all hashtags in corpus\n",
    "    hashtags = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(i)\n",
    "        for token in doc:\n",
    "            if token._.is_hashtag:\n",
    "                hashtags.append(token.text)\n",
    "    # count hashtags & return most frequents\n",
    "    return (\n",
    "        pd\n",
    "        .Series(hashtags)\n",
    "        .value_counts()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2bb882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_emojis(\n",
    "    corpus: list,\n",
    "    top: int = 5\n",
    ") -> pd.Series:\n",
    "    \"\"\"Retrieves the most frequent emojis in the given corpus.\"\"\"\n",
    "    # Retrieve emojis in corpus\n",
    "    emojis = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(i)\n",
    "        for token in doc:\n",
    "            if token._.is_emoji:\n",
    "                emojis.append(token.text)\n",
    "\n",
    "    # count occurrences & return most frequent\n",
    "    return (\n",
    "        pd\n",
    "        .Series(emojis)\n",
    "        .value_counts()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "257cbb0b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>#chanel</th>\n",
       "      <th>#fashion</th>\n",
       "      <th>#instagood</th>\n",
       "      <th>#beauty</th>\n",
       "      <th>#style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Hello friend, there are many products you need...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I couldn’t not kiss you, not sleep with you, n...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>We are a product wholesaler, we sell a lot of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zara jillstuart.jp ferragamo chanelofficial be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>We are a wholesaler, here has the best price, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>We are a product wholesaler, we sell a lot of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will be wearing this denim jacket on repeat th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>We are a wholesaler, here has the best price, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I am a wholesaler of brand products, if you ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Yes or No ? 🧡🧡👉👉kimberly.chanel.closet ​\\nDoub...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  #chanel  #fashion  \\\n",
       "57  Hello friend, there are many products you need...        0         0   \n",
       "5   I couldn’t not kiss you, not sleep with you, n...        1         1   \n",
       "42  We are a product wholesaler, we sell a lot of ...        0         0   \n",
       "11  zara jillstuart.jp ferragamo chanelofficial be...        1         0   \n",
       "35  We are a wholesaler, here has the best price, ...        0         0   \n",
       "38  We are a product wholesaler, we sell a lot of ...        0         0   \n",
       "4   Will be wearing this denim jacket on repeat th...        1         0   \n",
       "50  We are a wholesaler, here has the best price, ...        0         0   \n",
       "33  I am a wholesaler of brand products, if you ne...        0         0   \n",
       "25  Yes or No ? 🧡🧡👉👉kimberly.chanel.closet ​\\nDoub...        0         0   \n",
       "\n",
       "    #instagood  #beauty  #style  \n",
       "57           0        0       0  \n",
       "5            0        0       0  \n",
       "42           0        0       0  \n",
       "11           0        0       0  \n",
       "35           0        0       0  \n",
       "38           0        0       0  \n",
       "4            0        0       0  \n",
       "50           0        0       0  \n",
       "33           0        0       0  \n",
       "25           0        0       0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dummies(corpus: pd.Series,\n",
    "                   element: str,\n",
    "                   top: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create dummy encodings for most frequents text elements in the given corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : pd.Series\n",
    "        The corpus on which the elements will be searched for.\n",
    "    element : str, {'hashtag', 'emoji'}\n",
    "        The text element to look for. Currently, only 'hashtag' and 'emoji' are supported.\n",
    "    top : int, optional\n",
    "        The number of top modalities to dummy encode. Default is 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dummy encoding corresponding to the most frequent modalities of the specified elements.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError: if ``element`` is not supported.\n",
    "    \"\"\"\n",
    "    def _is_hashtag(token):\n",
    "        return token._.is_hashtag\n",
    "    def _is_emoji(token):\n",
    "        return token._.is_emoji\n",
    "    if element == 'hashtag':\n",
    "        detector = _is_hashtag\n",
    "    elif element == 'emoji':\n",
    "        detector = _is_emoji\n",
    "    else:\n",
    "        raise ValueError(\"Only 'hashtag' and 'emoji' elements are supported.\")\n",
    "\n",
    "    top_elements = (\n",
    "        corpus\n",
    "        .apply(lambda text: [token.text for token in nlp(text) if detector(token)])\n",
    "        .explode()\n",
    "        .value_counts()\n",
    "        .head(top)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    dummy = pd.DataFrame(index=corpus.index)\n",
    "    for e in top_elements:\n",
    "        dummy[e] = corpus.apply(lambda text: 1 if e in [token.text for token in nlp(text) if detector(token)] else 0)\n",
    "\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cd78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ratio(\n",
    "    corpus: list, \n",
    "    nlp: spacy.lang.en.English\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Computes the ratio of words to hashtags in the text column of the given dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe containing the text column.\n",
    "    nlp: spacy.lang.en.English\n",
    "        The spacy English pipeline.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        The list of word-to-hashtag ratios computed for each text in the dataframe.\n",
    "    \"\"\"\n",
    "    ratio = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(i)\n",
    "        nb_word = 0\n",
    "        nb_hash = 0\n",
    "        for token in doc:\n",
    "            if(token._.is_hashtag):\n",
    "                nb_hash+=1\n",
    "            else:\n",
    "                nb_word+=1\n",
    "        if((nb_hash+nb_word)!=0):\n",
    "            ratio.append(nb_word/(nb_hash+nb_word))\n",
    "        else:\n",
    "            ratio.append(0)  \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3bfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caps_ratio(\n",
    "    corpus: list,\n",
    "    nlp: spacy.lang.en.English\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Calculates the ratio of capitalized words in each text of the given DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: list\n",
    "        The list containing the text to analyze.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        The list containing the ratios of capitalized words for each text.\n",
    "    \"\"\"\n",
    "    ratio = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(clean_hashtag(i))\n",
    "        nb_lower = 0\n",
    "        nb_caps = 0\n",
    "        for token in doc:\n",
    "            if(token.text.isupper()):\n",
    "                nb_caps+=1\n",
    "            else:\n",
    "                nb_lower+=1\n",
    "        if((nb_caps+nb_lower)!=0):\n",
    "            ratio.append(nb_caps/(nb_caps+nb_lower))\n",
    "        else:\n",
    "            ratio.append(1)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd98a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_punct(\n",
    "        corpus: list,\n",
    "        nlp: spacy.lang.en.English\n",
    "        ) -> list:\n",
    "    \"\"\"\n",
    "    Counts the number of punctuation symbols in each string of the given corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list\n",
    "        A list of strings to be processed.\n",
    "    nlp : spacy.lang.en.English\n",
    "        A spaCy English language processing pipeline instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list containing the number of punctuation symbols in each string of the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    tot = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(clean_hashtag(i))\n",
    "        nb_punct = 0\n",
    "        for token in doc:\n",
    "            if(token.is_punct):\n",
    "                nb_punct+=1\n",
    "        tot.append(nb_punct)\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30eb08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_double(\n",
    "    corpus: list, \n",
    "    publication_time: list, \n",
    "    limit: float, \n",
    "    method: callable) -> list: \n",
    "    \"\"\"\n",
    "    Remove duplicated elements from a list of strings using Levenshtein distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of str\n",
    "        The list of strings to remove duplicates from.\n",
    "    publication_time : list of timestamp\n",
    "        The list of publication time for each element in the corpus.\n",
    "    limit : float\n",
    "        The distance threshold under which two elements are considered duplicates.\n",
    "        Must be in the range [0, 1] if using normalized Levenshtein distance, or\n",
    "        in the range [0, 100] if using classical Levenshtein distance.\n",
    "    method : function\n",
    "        The method to use to compute the distance between two strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        The list of strings with duplicates removed.\n",
    "    \"\"\"\n",
    "    def clean_hashtag(t): #--text\n",
    "        hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "        return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #\n",
    "    t = txt.copy()\n",
    "    distance = method #initialisiation de levenshtein avec la distance normalisée.\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        j=i+1\n",
    "        while(j<r):\n",
    "            if(distance(clean_hashtag(t[i]).strip(),clean_hashtag(t[j]).strip()) <= s ): # Si la distance entre les deux élemens de la liste inf à seuil\n",
    "                if(publication_time[i]<publication_time[j]):\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    del t[i]\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "            else:\n",
    "                j+=1\n",
    "        i+=1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd117ef",
   "metadata": {},
   "source": [
    "# Creation d'un sample de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "chanel_junk_valid = random.choices(chanel, k=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dcc7785",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chanel_junk_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextdistance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dist \u001b[38;5;241m=\u001b[39m textdistance\u001b[38;5;241m.\u001b[39mlevenshtein\u001b[38;5;241m.\u001b[39mnormalized_distance\n\u001b[1;32m----> 3\u001b[0m chanel_junk_valid_dd \u001b[38;5;241m=\u001b[39m del_double(\u001b[43mchanel_junk_valid\u001b[49m,publication_time_chanel,\u001b[38;5;241m0.5\u001b[39m,dist)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chanel_junk_valid' is not defined"
     ]
    }
   ],
   "source": [
    "import textdistance\n",
    "dist = textdistance.levenshtein.normalized_distance\n",
    "chanel_junk_valid_dd = del_double(chanel_junk_valid,publication_time_chanel,0.5,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chanel_junk_valid_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_df= pd.DataFrame()\n",
    "chanel_junk_valid_df['text'] = chanel_junk_valid_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_df.to_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_junk_valid.csv\")\n",
    "# Ajout de la variable is_junk sur excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6a2f0",
   "metadata": {},
   "source": [
    "### Sample de validation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e63044",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new = pd.read_excel('C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_junk_valid_new.xlsx')\n",
    "chanel_junk_valid_new = chanel_junk_valid_new[['text','is_junk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_right_side(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477dabf",
   "metadata": {},
   "source": [
    "# Ajout de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6099cc",
   "metadata": {},
   "source": [
    "### Ratio de mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3960316",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ratio(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add183c",
   "metadata": {},
   "source": [
    "### Ratio de caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_ratio(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a4d93",
   "metadata": {},
   "source": [
    "# Nb de ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_punct(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd114677",
   "metadata": {},
   "source": [
    "### Top hashtags junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4417d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hashtags(s,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef4c9d",
   "metadata": {},
   "source": [
    "### Top emojis junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbe051",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emojis(s,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a11e9",
   "metadata": {},
   "source": [
    "# Hashtag/emojis dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_emojis(chanel_junk_valid_new,5)\n",
    "dummy_hashtags(chanel_junk_valid_new,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55385e13",
   "metadata": {},
   "source": [
    "# Correlation, boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880884a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corr_df = chanel_junk_valid_new.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_df, annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467e8b3",
   "metadata": {},
   "source": [
    "Boxplot word ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de120e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=chanel_junk_valid_new, x=\"is_junk\", y=\"ratio_word\",color= 'skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a3277",
   "metadata": {},
   "source": [
    "Boxplot caps ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data  = chanel_junk_valid_new,\n",
    "            x     = \"is_junk\",\n",
    "            y     = \"ratio_caps\",\n",
    "            color = 'skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7950223",
   "metadata": {},
   "source": [
    "Boxplot nb ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=chanel_junk_valid_new, x=\"is_junk\", y=\"nb_punct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd2fd6",
   "metadata": {},
   "source": [
    "# Arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55af322",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk0 = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==0].index.values.tolist()\n",
    "i_0  = random.choices(junk0, k=round(len(junk0)*0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk1 = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==1].index.values.tolist()\n",
    "i_1  = random.choices(junk1, k=round(len(junk1)*0.40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a38bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = chanel_junk_valid_new.iloc[[*i_0,*i_1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chanel_junk_valid_new.drop(index = [*i_0,*i_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e180fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40372f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chanel_junk_valid_new['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chanel_junk_valid_new.drop(['text', 'is_junk'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chanel_junk_valid_new['is_junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = chanel_junk_valid_new.drop([1,2,3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89207583",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tree1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = y_test, y_pred = pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
