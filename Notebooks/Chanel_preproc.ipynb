{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a00d48c",
   "metadata": {},
   "source": [
    "### Corpus sur les hashtag Chanel, SpringSummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7915e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_11416\\727257464.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  chanel_5k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_5k.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2285"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chanel_5k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_5k.csv\",sep=\";\", error_bad_lines=False)\n",
    "chanel = chanel_5k[chanel_5k['language'] == 'en']['text'].tolist()\n",
    "len(chanel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c3709ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_11416\\4011020790.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  SS_5k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/SpringSummer_5k.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "SS_5k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/SpringSummer_5k.csv\",sep=\";\", error_bad_lines=False)\n",
    "SS = SS_5k[SS_5k['language'] == 'en']['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39e907a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x1c3ced09910>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacymoji import Emoji\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd485ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8892b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"hashtag\", first=True)\n",
    "hashtag_getter = lambda token: token.text[0] in (\"#\")\n",
    "Token.set_extension(\"is_hashtag\", getter=hashtag_getter, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646802c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_hashtags(text, N, top):\n",
    "    hashtag = []\n",
    "    index_vector = np.random.randint(len(text), size=N)\n",
    "    for i in index_vector:\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            if token._.is_hashtag:\n",
    "                hashtag.append(token.text)\n",
    "    Hashtag_corpus = pd.DataFrame()\n",
    "    Hashtag_corpus['hashtag'] = hashtag\n",
    "    Hashtag_corpus2 = Hashtag_corpus.groupby(\"hashtag\").size()\n",
    "    print(Hashtag_corpus2.nlargest(top).tail(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7b6ad",
   "metadata": {},
   "source": [
    "Top 20 des # dans les corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125350",
   "metadata": {},
   "source": [
    "Extraction du top 20 des # dans une selection al√©atoire de N post du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eedcdd54",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag\n",
      "#chanel              25\n",
      "#fashion             10\n",
      "#gucci                9\n",
      "#instagood            9\n",
      "#luxury               7\n",
      "#style                6\n",
      "#bag                  5\n",
      "#luxurylifestyle      5\n",
      "#shoes                5\n",
      "#chanellover          4\n",
      "#dior                 4\n",
      "#lv                   4\n",
      "#ootd                 4\n",
      "#pink                 4\n",
      "#Balenciaga           3\n",
      "#Belt                 3\n",
      "#Chanel               3\n",
      "#Men'sshoes           3\n",
      "#YvesSaintLaurent     3\n",
      "#aesthetic            3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_hashtags(chanel, 100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a857e17",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag\n",
      "#springsummer              12\n",
      "#SpringSummer               8\n",
      "#Firstcry                   6\n",
      "#FirstcryIndia              6\n",
      "#Firstcryfashion            6\n",
      "#Firstcryshopping           6\n",
      "#FussNowAtFirstcry          6\n",
      "#kidsfashion                6\n",
      "#kidswear                   6\n",
      "#shopatFirstcry             6\n",
      "#FirstcrySpringSummer23     5\n",
      "#incollaboration            5\n",
      "#newcollection              4\n",
      "#springfashion              4\n",
      "#springlaunch               4\n",
      "#fashion                    3\n",
      "#springiscoming             3\n",
      "#springsummercollection     3\n",
      "#2023                       2\n",
      "#eveningdress               2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_hashtags(SS, 25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2de82ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_emojis(text, N, top):\n",
    "    emoji = []\n",
    "    index_vector = np.random.randint(len(text), size=N)\n",
    "    for i in index_vector:\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            if token._.is_emoji:\n",
    "                emoji.append(token.text)\n",
    "    Emoji_corpus = pd.DataFrame()\n",
    "    Emoji_corpus['emoji'] = emoji\n",
    "    Emoji_corpus2 = Emoji_corpus.groupby(\"emoji\").size()\n",
    "    print(Emoji_corpus2.nlargest(top).tail(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616d372",
   "metadata": {},
   "source": [
    "Top 20 des emoji dans les corpus, m√™me proc√©d√© que pr√©cedemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "880a56bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "‚úà       3\n",
      "üéÅ       3\n",
      "üìû       3\n",
      "üëì       2\n",
      "üëú       2\n",
      "üëû       2\n",
      "üë†       2\n",
      "üë∂üèª      2\n",
      "‚òÄÔ∏è      1\n",
      "üáÆüáπ      1\n",
      "üíã       1\n",
      "üíñ       1\n",
      "üíô       1\n",
      "üíö       1\n",
      "üí≠       1\n",
      "üì∑       1\n",
      "üòÜ       1\n",
      "üòç       1\n",
      "üòù       1\n",
      "üôãüèº‚Äç‚ôÄ    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_emojis(chanel, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3836a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid = np.random.randint(len(chanel), size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5a4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chanel_junk_valid).to_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_junk_valid.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
