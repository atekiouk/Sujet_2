{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7971fb0",
   "metadata": {},
   "source": [
    "# Preprocessing des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56e37b",
   "metadata": {},
   "source": [
    "corpus de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "977c8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.tekiouk\\AppData\\Local\\Temp\\ipykernel_63048\\735203815.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus_sephora3k = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/corpus_sephora3k.csv\",sep=\";\", error_bad_lines=False)\n",
    "corpus_sephora3k.head()\n",
    "text = corpus_sephora3k[corpus_sephora3k['language'] == 'en']['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fbcb4",
   "metadata": {},
   "source": [
    "#### Extraction des emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6740f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clarascanner/blob/main/notebooks/cleaning.ipynb\n",
    "\n",
    "\n",
    "#------------------------ /!\\ Ne fonctionne pas------------------------------\n",
    "from emoji import UNICODE_EMOJI \n",
    "\n",
    "def emojis_extraction(text):\n",
    "    emojis = []\n",
    "    for x in text:\n",
    "        if x in UNICODE_EMOJI\n",
    "            emojis.append(x)\n",
    "            text = text.replace(x, f' {x} ')\n",
    "    return (text,emojis)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "import re\n",
    "def clean_emojis(txt): #--chaine de caract√®res\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "            u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U0001F1F2\"\n",
    "            u\"\\U0001F1F4\"\n",
    "            u\"\\U0001F620\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59ff1d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Swatches de la palette Pro Pigment Volume 3 de norvinacosmetics üß°\\nUne tr√®s bonne pigmentation üòç\\n\\n#norvinacosmetics #anastasiabeverlyhills anastasiabeverlyhills #norvinapalette #norvina norvina #norvinavolume3 #norvinacollection #norvinavol3 #swatches #swatch #swatcheyeshadow #swatchpalette #eyeshadowpalettecollection #eyeshadowpalette #makeupcollection #makeupaddict #makeuplover #makeuplovers #makeupaddiction #instabeauty #instamakeup #sephora sephora #sephorafrance sephorafrance'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4771877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Swatches de la palette Pro Pigment Volume 3 de norvinacosmetics üß°\\nUne tr√®s bonne pigmentation üòç\\n\\n  anastasiabeverlyhills   norvina                  sephora  sephorafrance',\n",
       " [])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis_extraction(clean_hashtag(text[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100cfe",
   "metadata": {},
   "source": [
    "#### Identification des doublons\n",
    "Afin d'√©viter d'analyser le texte de post identiques, nous allons ici supprimer les doublons pr√©sent dans notre corpus de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cc3d5",
   "metadata": {},
   "source": [
    "Pour ce faire nous allon utiliser la librairie textdistance (`pip install text-distance`), et nottament la distance normalis√©e `textdistance.hamming.normalized_distance` qui nous permettra de d√©finir un seuil entre 0 et 1.\n",
    "Nous pouvons √©galement utiliser la librairie `fuzzywuzzy`, qui va nous permettre gr√¢ce √† la fonction *token_sort_ratio* de mesurer la similitude entre deux chaines de caract√®res malgr√© la position des mots dans la chaines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15544dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea61527",
   "metadata": {},
   "source": [
    "On doit enlever les hashtags temporairement afin de v√©rifier s'il existe des doublons, car en effet des hashtag diff√©rent sur deux posts qui sont en fait identiques peuvent perturber le tri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea7e57",
   "metadata": {},
   "source": [
    "Fonction qui retourne notre texte sans les # : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a33b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_hashtag(t): #--text\n",
    "    hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "    return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f6fa5a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming(' a a a Je joue foot','Je joue foot a a a')\n",
    "#Compare mot √† mot la chaine de caract√®re ?\n",
    "#hamming(text[7],'The tea is HOT! Did you know that these 4 big beauty companies own the vast majority of beauty sales? Were you shocked to see that some of your favorite brands are owned by the same companies?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f309c",
   "metadata": {},
   "source": [
    "Une fois les hashtags retir√©, il nous faut calculer la distance entre tout les √©l√©ments du corpus. La fonction suivante va calculer la distance entre les √©lements de la liste (corpus de texte) et ensuite supprimer les √©lements qui sont pr√©sent plusieurs fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00d60ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ Renvoie une matrice de distance, pas opti\n",
    "#------------------------------------------------------------------------------------------\n",
    "def dist_mat(t): #-- text\n",
    "    dist_matrix = np.zeros((len(t),len(t)),dtype=np.float)\n",
    "    for i in range(len(t)):\n",
    "        for j in range(len(t)):\n",
    "            dist_matrix[i,j] = hamming(t[i],t[j])\n",
    "    return dist_matrix\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "def del_double(txt,s,method): # --text --seuil (s dans [0;1] ou [0;100] selon la m√©thode) --m√©thode (hamming normalized_distancee ou fuzzywuzzy-token_sort_ratio)\n",
    "    t = txt.copy()\n",
    "    if(method == \"hamming-normalized_distance\"): #Ici deux texte identique dist = 0\n",
    "        hamming = textdistance.hamming.normalized_distance #initialisiation de hamming avec la distance normalis√©e.\n",
    "        i = 0\n",
    "        r = len(t)\n",
    "        while(i<r):\n",
    "            r = len(t)\n",
    "            j=i+1\n",
    "            while(j<r):\n",
    "                if(hamming(clean_emojis(clean_hashtag(t[i])),clean_emojis(clean_hashtag(t[j]))) <= s ): # Si la distance entre les deux √©lemens de la liste inf √† seuil\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    if(method == \"fuzzywuzzy-token_sort_ratio\"): #Ici deux texte identique ratio = 100\n",
    "        i = 0\n",
    "        r = len(t)\n",
    "        while(i<r):\n",
    "            r = len(t)\n",
    "            j=i+1\n",
    "            while(j<r):\n",
    "                if(token_sort_ratio(clean_emojis(clean_hashtag(t[i])),clean_emojis(clean_hashtag(t[j]))) >= s ): # Si la distance entre les deux √©lemens de la liste inf √† seuil\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    return t\n",
    "\n",
    "# Supprimer les doublons en fonction du timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbb645",
   "metadata": {},
   "source": [
    "Suppression des posts identifi√©s comme doublons (exemple) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bab4b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour a tous #Hello', 'Je joue au foot', 'Je suis au bureau #lalala']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ['Bonjour a tous #Hello','Bonjour a tous #Lundi' , 'Je joue au foot' ,'Je suis au bureau #lalala', 'Je suis au bureau #lalala']\n",
    "del_double(t, 0.40, \"hamming-normalized_distance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1f2e325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour a tous #Hello', 'Je joue au foot', 'Je suis au bureau #lalala']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ['Bonjour a tous #Hello','Bonjour a tous #Lundi' , 'Je joue au foot' ,'Je suis au bureau #lalala', 'Je suis au bureau #lalala']\n",
    "del_double(t, 90, \"fuzzywuzzy-token_sort_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c585493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sort_ratio('hello','hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09fb9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "Token_Sort_Ratio = token_sort_ratio('The tea is HOT! Did you know that these 4 big beauty companies own the vast majority of beauty sales? Were you shocked to see that some of your favorite brands are owned by the same companies?'\n",
    "                                    ,'Hey everyone, The tea is HOT! Did you know that these 4 big beauty companies own the vast majority of beauty sales? Were you shocked to see that some of your favorite brands are owned by the same companies?')\n",
    "print(Token_Sort_Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "caba6f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full glam to sit in my dressing gown & I‚Äôd have it no other wayüßñüèº\\u200d‚ôÄÔ∏è']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [text[2],text[2]]\n",
    "del_double(t, 90, \"fuzzywuzzy-token_sort_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052be63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a006b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a35428",
   "metadata": {},
   "source": [
    "#### Ratio mot/hashtag\n",
    "Calcul du ratio mot/hastag dans un post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b42db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3b69",
   "metadata": {},
   "source": [
    "(OPTIONEL) Ici nous ajoutons le token 'hashtag' dans la pipe du mod√®le spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6a638b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.hashtag_pipe(doc)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  /!\\ Ne marche pas\n",
    "#--------------------------------------------------------------------------------------------\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.head.text) + 1\n",
    "                    print(start_index, end_index)\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        if retokenizer.merge(doc[start_index:end_index]) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"hashtag\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ab37d",
   "metadata": {},
   "source": [
    "Afin de calculer le ration mot/hashtags, nous allons claculer le nombre de mot, cependant en utilisant la pipe spacy classique, un '#' est un token en lui-m√™me. Nous devons donc calculer le nom de de '#' et le soustraire au nombre total de mot, car en effet tout les mot pr√©c√©d√© d'un '#' seront consid√©r√© comme de \"vrai' mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e251298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "def del_junk_post(txt,s): # --text --seuil (s dans [0;1], le seuil 1 permet de supprimer un post compos√© √† 100% de hashtag)\n",
    "    t = txt.copy()\n",
    "    nlp = English()\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        nb_hash = 0\n",
    "        nb_word = 0\n",
    "        doc = nlp(t[i]) # On tokenize l'element de notre liste\n",
    "        for token in doc: # Pour chaque token de la liste\n",
    "            if(token.text == '#'):\n",
    "                nb_hash+=1\n",
    "            else:\n",
    "                nb_word+=1\n",
    "        if(nb_word-nb_hash==0): # On v√©rifie d'abord que le nombre de mot n'est pas nul, auquel cas nous supprimmons le post\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        elif(nb_hash/(nb_word-nb_hash) > s): # On calcul le ratio, si inf√©rieur au seuil => on delete\n",
    "            del t[i]\n",
    "            r = len(t)\n",
    "        i+=1\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5bbb3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour √† tous', 'Salut', 'tutu tata toto']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = ['a #salut #bonjour #hello', 'Bonjour √† tous','je je je je #salut #bonjour #hello','Salut','#bjr #llala #kdkdd', 'tutu tata toto']\n",
    "del_junk_post(tst,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a12d602d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4778179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b2e1807",
   "metadata": {},
   "source": [
    "#### Traitement des hashtag restant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f74223bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0258ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'i', 'love', 'sep', 'hora']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordninja.split('helloilovesephora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ad791",
   "metadata": {},
   "outputs": [],
   "source": [
    " #/!\\ Custom le language en ajoutant les marques !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd39bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f0b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e107723",
   "metadata": {},
   "source": [
    "check les seuils , d√©composer les mot dans les hashtag, position du hashtag dans le post, dictionnaire frequence langue anglaise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1fa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e48141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c4063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8960de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0a77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59535933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61459d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508540cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bda77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20096786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe47b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa9b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d04cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49212a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
