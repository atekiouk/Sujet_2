{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675abf8a",
   "metadata": {},
   "source": [
    "# Import des Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4ac8d",
   "metadata": {},
   "source": [
    "On importe toutes les bibliothÃ¨ques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f79094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9c430",
   "metadata": {},
   "source": [
    "Importation du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3957c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# documents in corpus: 2285\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_5k.csv\", sep=\";\", parse_dates=[\"publication_time\"])\n",
    "\n",
    "# on rÃ©cupÃ©re les posts en anglais\n",
    "mask = corpus[\"language\"] == 'en'\n",
    "corpus = corpus.loc[mask]\n",
    "chanel = corpus['text'].tolist()\n",
    "publication_time_chanel = corpus['publication_time'].tolist()\n",
    "\n",
    "print(f\"# documents in corpus: {len(chanel)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7fa61",
   "metadata": {},
   "source": [
    "On dÃ©finit les diffÃ©rents Ã©lÃ©ments de la *pipeline* `spacy` pour dÃ©tecter les emojis et les hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44faad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"hashtag\")\n",
    "def hashtag_pipe(doc):\n",
    "    \"\"\"Spacy pipe that can detect if a word is a hashtag or not.\"\"\"\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token in doc:\n",
    "            if token.text == '#':\n",
    "                if token.head is not None:\n",
    "                    start_index = token.i\n",
    "                    end_index = start_index + 1\n",
    "                    with doc.retokenize() as retokenizer:\n",
    "                        retokenizer.merge(doc[start_index:end_index+1])\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "\n",
    "# dÃ©finition du pipe\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe(\"hashtag\", first=True)\n",
    "Token.set_extension(\"is_hashtag\", getter=lambda token: token.text[0] in (\"#\"), force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1fbf0",
   "metadata": {},
   "source": [
    "# MÃ©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_right_side(df):\n",
    "        for i in range(len(df['text'])):\n",
    "            if(df['text'][i][-1]==\"#\"):\n",
    "                df['text'][i] = df['text'][i].rstrip(df['text'][i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a3e567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#chanel       12\n",
       "#beauty        3\n",
       "#chanelbag     3\n",
       "#makeup        3\n",
       "#instagood     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: voir si l'on peut passer en paramÃ¨tre un objet spacy qui identifie le type de token,\n",
    "# afin de n'Ã©crire qu'une seule mÃ©thode (pour les hashtags, les emojis, etc...).\n",
    "def top_hashtags(\n",
    "    corpus: list,\n",
    "    top: int = 5\n",
    ") -> pd.Series:\n",
    "    \"\"\"Retrieves the most frequent hashtags in the given corpus.\"\"\"\n",
    "    # retrieve all hashtags in corpus\n",
    "    hashtags = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(i)\n",
    "        for token in doc:\n",
    "            if token._.is_hashtag:\n",
    "                hashtags.append(token.text)\n",
    "    \n",
    "    # count hashtags & return most frequents\n",
    "    return (\n",
    "        pd\n",
    "        .Series(hashtags)\n",
    "        .value_counts()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top)\n",
    "    )\n",
    "\n",
    "top_hashtags(corpus=corpus[\"text\"].iloc[:50], top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2bb882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ðŸ“ž    8\n",
       "ðŸŽ    8\n",
       "âœˆ    8\n",
       "â¤    7\n",
       "ðŸ‘œ    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_emojis(\n",
    "    corpus: list,\n",
    "    top: int = 5\n",
    ") -> pd.Series:\n",
    "    \"\"\"Retrieves the most frequent emojis in the given corpus.\"\"\"\n",
    "    # Retrieve emojis in corpus\n",
    "    emojis = []\n",
    "    for i in corpus:\n",
    "        doc = nlp(i)\n",
    "        for token in doc:\n",
    "            if token._.is_emoji:\n",
    "                emojis.append(token.text)\n",
    "\n",
    "    # count occurrences & return most frequent\n",
    "    return (\n",
    "        pd\n",
    "        .Series(emojis)\n",
    "        .value_counts()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top)\n",
    "    )\n",
    "\n",
    "top_emojis(corpus=corpus[\"text\"].iloc[:50], top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71331f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>#chanel</th>\n",
       "      <th>#fashion</th>\n",
       "      <th>#instagood</th>\n",
       "      <th>#beauty</th>\n",
       "      <th>#style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Hello friend, there are many products you need...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I couldnâ€™t not kiss you, not sleep with you, n...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>We are a product wholesaler, we sell a lot of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zara jillstuart.jp ferragamo chanelofficial be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>We are a wholesaler, here has the best price, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>We are a product wholesaler, we sell a lot of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will be wearing this denim jacket on repeat th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>We are a wholesaler, here has the best price, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I am a wholesaler of brand products, if you ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Yes or No ? ðŸ§¡ðŸ§¡ðŸ‘‰ðŸ‘‰kimberly.chanel.closet â€‹\\nDoub...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  #chanel  #fashion  \\\n",
       "57  Hello friend, there are many products you need...        0         0   \n",
       "5   I couldnâ€™t not kiss you, not sleep with you, n...        1         1   \n",
       "42  We are a product wholesaler, we sell a lot of ...        0         0   \n",
       "11  zara jillstuart.jp ferragamo chanelofficial be...        1         0   \n",
       "35  We are a wholesaler, here has the best price, ...        0         0   \n",
       "38  We are a product wholesaler, we sell a lot of ...        0         0   \n",
       "4   Will be wearing this denim jacket on repeat th...        1         0   \n",
       "50  We are a wholesaler, here has the best price, ...        0         0   \n",
       "33  I am a wholesaler of brand products, if you ne...        0         0   \n",
       "25  Yes or No ? ðŸ§¡ðŸ§¡ðŸ‘‰ðŸ‘‰kimberly.chanel.closet â€‹\\nDoub...        0         0   \n",
       "\n",
       "    #instagood  #beauty  #style  \n",
       "57           0        0       0  \n",
       "5            0        0       0  \n",
       "42           0        0       0  \n",
       "11           0        0       0  \n",
       "35           0        0       0  \n",
       "38           0        0       0  \n",
       "4            0        0       0  \n",
       "50           0        0       0  \n",
       "33           0        0       0  \n",
       "25           0        0       0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dummies(\n",
    "    corpus: pd.Series,\n",
    "    element: str,\n",
    "    top: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create dummy encodings for most frequents text elements in the given corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: pd.Series\n",
    "        The corpus on which the elements will be searched for.\n",
    "    element: str, {'hashtag', 'emoji'}\n",
    "        The text element to look for. Currently, only 'hashtag' and 'emoji' are supported.\n",
    "    top: int, defaults to 5\n",
    "        The number of top modalities to dummy encode.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dummy encoding corresponding to the most frequent modalities of the specified elements.\n",
    "    \"\"\"\n",
    "    if element == 'hashtag':\n",
    "        regex = r'\\#\\w+'\n",
    "    elif element == 'emoji':\n",
    "        regex = r'[^\\w\\s,]'\n",
    "    else:\n",
    "        raise ValueError(\"Only 'hashtag' and 'emoji' elements are supported.\")\n",
    "\n",
    "    all_elements = corpus.str.findall(regex).explode()\n",
    "    top_elements = all_elements.value_counts().head(top).index\n",
    "\n",
    "    dummy = pd.DataFrame(index=corpus.index)\n",
    "    for e in top_elements:\n",
    "        dummy[e] = corpus.str.contains(e).astype(int)\n",
    "    \n",
    "    return dummy\n",
    "\n",
    "\n",
    "temp = corpus.iloc[:50]\n",
    "(\n",
    "    pd.concat(\n",
    "        (temp[[\"text\"]], create_dummies(corpus=temp[\"text\"], element=\"hashtag\")),\n",
    "        axis=1\n",
    "    )\n",
    "    .sample(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d59e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(corpus: pd.Series, element: str, top: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create dummy encodings for most frequents text elements in the given corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : pd.Series\n",
    "        The corpus on which the elements will be searched for.\n",
    "    element : str, {'hashtag', 'emoji'}\n",
    "        The text element to look for. Currently, only 'hashtag' and 'emoji' are supported.\n",
    "    top : int, optional\n",
    "        The number of top modalities to dummy encode. Default is 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dummy encoding corresponding to the most frequent modalities of the specified elements.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError: if ``element`` is not supported.\n",
    "    \"\"\"\n",
    "    def _is_hashtag(token):\n",
    "        return token._.is_hashtag\n",
    "    def _is_emoji(token):\n",
    "        return token._.is_emoji\n",
    "    if element == 'hashtag':\n",
    "        detector = _is_hashtag\n",
    "    elif element == 'emoji':\n",
    "        detector = _is_emoji\n",
    "    else:\n",
    "        raise ValueError(\"Only 'hashtag' and 'emoji' elements are supported.\")\n",
    "\n",
    "    top_elements = (\n",
    "        corpus\n",
    "        .apply(lambda text: [token.text for token in nlp(text) if detector(token)])\n",
    "        .explode()\n",
    "        .value_counts()\n",
    "        .head(top)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    dummy = pd.DataFrame(index=corpus.index)\n",
    "    for e in top_elements:\n",
    "        dummy[e] = corpus.apply(lambda text: 1 if e in [token.text for token in nlp(text) if detector(token)] else 0)\n",
    "\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_hashtags(df, top):\n",
    "#     hashtag = []\n",
    "#     for i in df[df['is_junk']==1]['text']:\n",
    "#         doc = nlp(i)\n",
    "#         for token in doc:\n",
    "#             if token._.is_hashtag:\n",
    "#                 hashtag.append(token.text)\n",
    "#     Hashtag_corpus = pd.DataFrame()\n",
    "#     Hashtag_corpus['hashtag'] = hashtag\n",
    "#     Hashtag_corpus2 = Hashtag_corpus.groupby(\"hashtag\").size().nlargest(top).tail(top)\n",
    "#     df_hash = pd.DataFrame(Hashtag_corpus2.index)\n",
    "#     print(df_hash)\n",
    "#     j=0\n",
    "#     for e in df_hash['hashtag']:\n",
    "#         dummy = []\n",
    "#         for t in df['text']:\n",
    "#             if(e in t):\n",
    "#                 dummy.append(1)\n",
    "#             else:\n",
    "#                 dummy.append(0)\n",
    "#         df['has_hash_'+str(j)] = dummy\n",
    "#         j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae810af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_emojis(df,top):\n",
    "#     emoji = []\n",
    "#     for i in df[df['is_junk']==1]['text']:\n",
    "#         doc = nlp(i)\n",
    "#         for token in doc:\n",
    "#             if token._.is_emoji:\n",
    "#                 emoji.append(token.text)\n",
    "#     Emoji_corpus = pd.DataFrame()\n",
    "#     Emoji_corpus['emoji'] = emoji\n",
    "#     Emoji_corpus2= pd.DataFrame(Emoji_corpus.groupby(\"emoji\").size().nlargest(top).tail(top))\n",
    "#     df_emo = pd.DataFrame(Emoji_corpus2.index)\n",
    "#     j=0\n",
    "#     for e in df_emo['emoji']:\n",
    "#         dummy = []\n",
    "#         for t in df['text']:\n",
    "#             if(e in t):\n",
    "#                 dummy.append(1)\n",
    "#             else:\n",
    "#                 dummy.append(0)\n",
    "#         df['has_emo_'+str(j)] = dummy\n",
    "#         j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ratio(df):\n",
    "    ratio = []\n",
    "    for i in df['text']:\n",
    "        doc = nlp(i)\n",
    "        nb_word = 0\n",
    "        nb_hash = 0\n",
    "        for token in doc:\n",
    "            if(token._.is_hashtag):\n",
    "                nb_hash+=1\n",
    "            else:\n",
    "                nb_word+=1\n",
    "        if((nb_hash+nb_word)!=0):\n",
    "            ratio.append(nb_word/(nb_hash+nb_word))\n",
    "        else:\n",
    "            ratio.append(0)  \n",
    "    df['ratio_word'] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caps_ratio(df):\n",
    "    ratio = []\n",
    "    for i in df['text']:\n",
    "        doc = nlp(clean_hashtag(i))\n",
    "        nb_lower = 0\n",
    "        nb_caps = 0\n",
    "        for token in doc:\n",
    "            if(token.text.isupper()):\n",
    "                nb_caps+=1\n",
    "            else:\n",
    "                nb_lower+=1\n",
    "        if((nb_caps+nb_lower)!=0):\n",
    "            ratio.append(nb_caps/(nb_caps+nb_lower))\n",
    "        else:\n",
    "            ratio.append(1)\n",
    "    df['ratio_caps'] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd98a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_punct(df):\n",
    "    tot = []\n",
    "    for i in df['text']:\n",
    "        doc = nlp(clean_hashtag(i))\n",
    "        nb_punct = 0\n",
    "        for token in doc:\n",
    "            if(token.is_punct):\n",
    "                nb_punct+=1\n",
    "        tot.append(nb_punct)\n",
    "    df['nb_punct'] = tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_hashtag(t): #--text\n",
    "    hashtag_pattern= re.compile(\"#[A-Za-z0-9_]+\")\n",
    "    return re.sub(hashtag_pattern,\"\", t) #On supprime tout les types de #\n",
    "\n",
    "\n",
    "def del_double(txt,publication_time,s,method): # --text --liste des date et heure de publication pour chaque Ã©lÃ©ment de la liste --seuil (s dans [0;1] ou [0;100] selon la mÃ©thode) --mÃ©thode\n",
    "    t = txt.copy()\n",
    "    distance = method #initialisiation de levenshtein avec la distance normalisÃ©e.\n",
    "    i = 0\n",
    "    r = len(t)\n",
    "    while(i<r):\n",
    "        r = len(t)\n",
    "        j=i+1\n",
    "        while(j<r):\n",
    "            if(distance(clean_hashtag(t[i]).strip(),clean_hashtag(t[j]).strip()) <= s ): # Si la distance entre les deux Ã©lemens de la liste inf Ã  seuil\n",
    "                if(publication_time[i]<publication_time[j]):\n",
    "                    del t[j] #delete\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "                else:\n",
    "                    del t[i]\n",
    "                    r = len(t) #on actualise la taille de la listes\n",
    "            else:\n",
    "                j+=1\n",
    "        i+=1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd117ef",
   "metadata": {},
   "source": [
    "# Creation d'un sample de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "chanel_junk_valid = random.choices(chanel, k=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "dist = textdistance.levenshtein.normalized_distance\n",
    "chanel_junk_valid_dd = del_double(chanel_junk_valid,publication_time_chanel,0.5,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chanel_junk_valid_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_df= pd.DataFrame()\n",
    "chanel_junk_valid_df['text'] = chanel_junk_valid_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_df.to_csv(\"C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_junk_valid.csv\")\n",
    "# Ajout de la variable is_junk sur excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6a2f0",
   "metadata": {},
   "source": [
    "### Sample de validation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e63044",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new = pd.read_excel('C:/Users/a.tekiouk/Sujet_2/Sujet_2/DATA/chanel_junk_valid_new.xlsx')\n",
    "chanel_junk_valid_new = chanel_junk_valid_new[['text','is_junk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_right_side(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477dabf",
   "metadata": {},
   "source": [
    "# Ajout de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6099cc",
   "metadata": {},
   "source": [
    "### Ratio de mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3960316",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ratio(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add183c",
   "metadata": {},
   "source": [
    "### Ratio de caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_ratio(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a4d93",
   "metadata": {},
   "source": [
    "# Nb de ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_punct(chanel_junk_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanel_junk_valid_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd114677",
   "metadata": {},
   "source": [
    "### Top hashtags junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4417d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hashtags(s,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef4c9d",
   "metadata": {},
   "source": [
    "### Top emojis junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbe051",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emojis(s,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a11e9",
   "metadata": {},
   "source": [
    "# Hashtag/emojis dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_emojis(chanel_junk_valid_new,5)\n",
    "dummy_hashtags(chanel_junk_valid_new,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55385e13",
   "metadata": {},
   "source": [
    "# Correlation, boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880884a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_df = chanel_junk_valid_new.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_df, annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467e8b3",
   "metadata": {},
   "source": [
    "Boxplot word ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de120e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=chanel_junk_valid_new, x=\"is_junk\", y=\"ratio_word\",color= 'skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a3277",
   "metadata": {},
   "source": [
    "Boxplot caps ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data  = chanel_junk_valid_new,\n",
    "            x     = \"is_junk\",\n",
    "            y     = \"ratio_caps\",\n",
    "            color = 'skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7950223",
   "metadata": {},
   "source": [
    "Boxplot nb ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=chanel_junk_valid_new, x=\"is_junk\", y=\"nb_punct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd2fd6",
   "metadata": {},
   "source": [
    "# Arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import random\n",
    "from   sklearn.tree            import DecisionTreeClassifier\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.metrics         import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55af322",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk0 = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==0].index.values.tolist()\n",
    "i_0  = random.choices(junk0, k=round(len(junk0)*0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk1 = chanel_junk_valid_new[chanel_junk_valid_new['is_junk']==1].index.values.tolist()\n",
    "i_1  = random.choices(junk1, k=round(len(junk1)*0.40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a38bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = chanel_junk_valid_new.iloc[[*i_0,*i_1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chanel_junk_valid_new.drop(index = [*i_0,*i_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e180fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40372f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chanel_junk_valid_new['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chanel_junk_valid_new.drop(['text', 'is_junk'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chanel_junk_valid_new['is_junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = chanel_junk_valid_new.drop([1,2,3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89207583",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tree1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = y_test, y_pred = pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
